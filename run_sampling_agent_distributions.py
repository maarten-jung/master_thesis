#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@author: Maarten l. Jung

Runs the Bayesian independence sampler with prior distributions and 
likelihood functions generated by the Bayesian habit-learning agent.
"""

import numpy as np
import json
import jsonpickle
import jsonpickle.ext.numpy as jsonpickle_numpy
from joblib import Parallel, delayed
from pathlib import Path

jsonpickle_numpy.register_handlers()


def calc_ess(samples, step_size = 1, max_lag = 2000):
    # based on BEAST https://beast.community/
    # code adapted from the GitHub page (commit d2b36cf)
    # https://github.com/beast-dev/beast-mcmc/blob/f62cf395998ce1cd0538412177a13e672f96e9ac/src/dr/inference/trace/TraceCorrelation.java
    #
    # The effective samples size (ess) is defined as the number of samples devided by the (integrated) autocorrelation time (act).
    # The act is the delay at which the autocorrelation between the sample sequence and the delayed/shifted sample sequence is zero.
    # The act is calculated by first finding the approx. point where the autocovariance function (acf) is zero (indicated by the sum of 
    # adjacent values being less than zero). The acf is then assumed to be approx. linear and the act can thus be approximated by twice 
    # the area under the acf curve divided by the value of the acf at delay = 0.
    
    num_samples = len(samples)
    
    assert num_samples >= 10, "At least 10 samples needed!"
    
    if type(samples) is not np.ndarray:
        samples = np.array(samples)
    
    biggest_lag = min(num_samples-1, max_lag)
    
    # autocovariance function
    autocov = np.zeros(biggest_lag, dtype = np.float64)
    
    cent_samples = samples - samples.mean()
    
    for lag in range(biggest_lag):
      
        autocov[lag] = (cent_samples[:num_samples-lag] @ cent_samples[lag:]) / (num_samples - lag)
        
        if lag == 0: # (autocovariance at lag 0) == variance of samples
            integrated_autocov = autocov[lag]
        elif lag % 2 == 0:
            # sum of adjacent pairs of autocovariances must be positive (Geyer, 1992)
            sum_adj_pairs = autocov[lag-1] + autocov[lag]
            if sum_adj_pairs > 0:
                integrated_autocov += 2.0 * sum_adj_pairs
            else:
                break
        
    # integrated autocorrelation time
    if autocov[0] == 0:
        act = 0
    else:
        act = (step_size * integrated_autocov) / autocov[0]
    
    # effective sample size  
    if act == 0:
        ess = 1
    else:
        ess = (step_size * num_samples) / act
    
    return ess


def bayes_indep_sampler(prior, likelihood, ess_crit = 500, max_iter = 10**5):
    
    if type(likelihood) is not np.ndarray:
        likelihood = np.array(likelihood, dtype = np.float64) 
     
    prior_draws = np.random.choice(len(prior), size = max_iter + 1, replace = True, p = prior)
    lh_prior_draws = likelihood[prior_draws]
    uniform_draws = np.random.uniform(size = max_iter)
    
    posterior_samples = np.zeros(max_iter, dtype = np.int32)    
    pos_last_accepted = 0
   
    for i in range(max_iter):
        acceptance_value = lh_prior_draws[i+1] / lh_prior_draws[pos_last_accepted]
        if uniform_draws[i] < acceptance_value:
            pos_last_accepted = i + 1
        posterior_samples[i] = prior_draws[pos_last_accepted]
        
        if i >= ess_crit:
            if calc_ess(posterior_samples[:i+1]) >= ess_crit:
                return posterior_samples[:i+1]
   
    return posterior_samples


# Run sampling ---------------------------------------------------------------

folder = Path.cwd() / "data"

alpha_values = [1, 10, 100]
num_mb = 50
num_rep = 100
ess_crit = 500

dict_sampling = {key: [] for key in ["habitual_tendency", "posterior_samples", "n_posterior_samples"]}
for a, alpha in enumerate(alpha_values):
    fname_in = folder / f"worlds_alpha_{alpha}_mb_50_rep_100.json"
    with open(fname_in, 'r') as file_in:
        data = json.load(file_in)    
    worlds = jsonpickle.decode(data)
    pos_samples = [None]*num_rep 
    n_pos_samples = np.zeros((num_rep, num_mb), dtype = np.int32)
    for rep in range(num_rep):
        print(f"\nhabit {a+1} out of {len(alpha_values)}, repetition {rep+1} out of {num_rep}\n")
        
        prior = worlds[rep].agent.prior_policies_all[:, :, 0]
        lik = worlds[rep].agent.likelihood[:, 0, :, 0] # (unnormalized) likelihoods of 1st time steps
        pos_samples[rep] = Parallel(n_jobs = -1, verbose = 50) \
                                   (delayed(bayes_indep_sampler)(prior[mb, :], lik[mb, :], ess_crit) for mb in range(num_mb))
        n_pos_samples[rep, :] = np.array([len(samples) for samples in pos_samples[rep]], dtype = np.int32)        
        
    dict_sampling["habitual_tendency"].append(1.0 / alpha)
    dict_sampling["posterior_samples"].append(pos_samples)
    dict_sampling["n_posterior_samples"].append(n_pos_samples)

# Save sampling data ---------------------------------------------------------

fname_out = folder / f"sampling_ess_crit_{ess_crit}_alpha_{'_'.join(map(str, alpha_values))}_mb_{num_mb}_rep_{num_rep}.json"         
pickled = jsonpickle.encode(dict_sampling)
with open(fname_out, 'w') as file_out:
    json.dump(pickled, file_out)    

